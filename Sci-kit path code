import pandas as pd
import time
import datetime
import numpy as np
import csv
import seaborn as sns
import itertools
import scipy.optimize as opt
import pylab as pl
import math
import matplotlib.pyplot as plt
import matplotlib as mpl

from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error
from sklearn.metrics import log_loss
from sklearn.metrics import roc_auc_score
from sklearn.metrics import make_scorer
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import jaccard_score
from sklearn.metrics import classification_report

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_predict

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier

from catboost import CatBoostClassifier, Pool

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split

# timer on
start_time = datetime.datetime.now()

# data loading
#df = pd.read_csv(r'DATA_csv\VHS_output.csv', index_col='match_id')
#df_test = pd.read_csv(r'DATA_csv\VHS_output_test.csv', index_col='match_id')

df = pd.read_csv(r'DATA_csv\features.csv', index_col='match_id')
df_test = pd.read_csv(r'DATA_csv\features_test.csv', index_col='match_id')

# excessiv data

del df['duration']
del df['tower_status_radiant']
del df['tower_status_dire']
del df['barracks_status_radiant']
del df['barracks_status_dire']

#df.fillna(value=df.median(), inplace=True)
df.fillna(value=0, inplace=True)

# preprocessing
# switch r and d teams, with coordinat fix
def data_doubling(df):
    df2 = df.copy(deep=True)
    df2['radiant_win']=df2['radiant_win'].map({1: 0, 0: 1})
    df2_col =df2.columns.tolist()
    j = 0
    # assigning hero columns
    for col in df2.columns:
        for i in range(5):
            if col.count('r%d' %(i+1))>0: 
                col1 = col.replace('r%d' %(i+1),'d%d' %(i+1))
                df2_col[j]=col1
            elif col.count('d%d' %(i+1))>0:
                col1 = col.replace('d%d' %(i+1),'r%d' %(i+1))
                df2_col[j]=col1
        j+=1
    
    # assigning team 0/1 columns
    #['tower_kill_team','aegis_team','tower_deny_team','first_blood_team']
    for field in ['first_blood_team']:
        df2[field] = df2[field].map({1:0,0:1})
    
    # assigning items column
    cols =[]
    for col in df.columns:
        if col.count('dire')>0:
            cols.append(col.replace('dire',''))
    for field in cols:
        df2['tmp']=df2['radiant%s' % field]
        df2['radiant%s' % field] = df2['dire%s' % field]
        df2['dire%s' % field] = df2['tmp']
    del df2['tmp']
    
    # assigning player
    
    df2.columns = df2_col           
    temp_index = list(df2.index)
    temp_index = [x+200000 for x in temp_index]
    df2.index = temp_index
    df = pd.concat([df,df2])
    return df

cols =[]
for col in df.columns:
    if col.count('dire')>0:
        cols.append(col.replace('dire',''))
        

# add team fields
def add_team_2 (df,fields):
    
    for c in fields:
        r_columns = [f'r{i}{c}' for i in range(1, 6)]
        d_columns = [f'd{i}{c}' for i in range(1, 6)]
        
        df['r_std' + c] = df[r_columns].std(1)
        df['d_std' + c] = df[d_columns].std(1)
        df['std' + c + 'ratio'] = df['r_std' + c] / (df['d_std' + c]+1)
        
        df['r_mean' + c] = df[r_columns].mean(1)
        df['d_mean' + c] = df[d_columns].mean(1)
        df['mean' + c + '_ratio'] = df['r_mean' + c] / (df['d_mean' + c]+1)
        
        #df['r_time'+c] = df[r_columns].mean(1)/(df['game_time']+1)
        #df['d_time'+c] = df[d_columns].mean(1)/(df['game_time']+1)
        #df['time'+c+'_ratio'] = df['r_time'+c]/(df['d_time'+c]+1)
        
    
# one-hot encoding of hero id +1 for radiant, -1 for dire    
def heroes_encoding(df):
    N = 121
    X_pick = np.zeros((df.shape[0], N))
    ii=0
    for i in df.index:
        for p in range(5):
            X_pick[ii, int(df.loc[i, 'r%d_hero' % (p+1)])] = 1
            X_pick[ii, int(df.loc[i, 'd%d_hero' % (p+1)])] = -1
        ii+=1
    for i in range(N):
        df['hero_%d' % i]=X_pick[:,i]
    for i in range(5):
        del df['r%d_hero' % (i+1)]
        del df['d%d_hero' % (i+1)]
        

def item_deletion(df,fields):
    for field in fields:
        del df['radiant%s' %field]
        del df['dire%s' %field]

# DATA PREPARATION

# fileds combining
fields = ['_kills', '_deaths', '_gold', '_lh', '_xp', '_level','_items',]
#'_buybacks' - deleted

# data processing, normalization and split

# item fields to delete
#'_bottle_time','_courier_time','_flying_courier_time', '_tpscroll_count', 
#'_boots_count', '_ward_observer_count','_ward_sentry_count', '_first_ward_time'
fields2 = ['_tpscroll_count','_boots_count', '_ward_observer_count','_ward_sentry_count']

df = data_doubling(df)
heroes_encoding(df)
add_team_2(df,fields)
item_deletion(df,fields2)

df.fillna(value=0, inplace=True)
#df.fillna(value=df.median(), inplace=True)

y = df['radiant_win']
del df['radiant_win']

del df['first_blood_player1']
del df['first_blood_player2']

#del df['aegis_team']
#del df['tower_deny_team']
#del df['first_blood_team']
#del df['aegis_time']
#del df['tower_deny_time']
#del df['first_blood_time']
 
# normalized data
X = preprocessing.StandardScaler().fit(df).transform(df)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=241)

# non-normalized data
X_cat = df.values
X_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(X_cat, y, test_size=0.1, random_state=241)

# test data
heroes_encoding(df_test)
add_team_2(df_test,fields)
df_test.fillna(value=0, inplace=True)

# normilized test data
XX = preprocessing.StandardScaler().fit(df_test).transform(df_test)

# non-normilized test data
XX = df_test.values

print('Time elapsed, data prep:', datetime.datetime.now() - start_time)


# Simple Logistic Regression
start_time_l = datetime.datetime.now()
LR = LogisticRegression(C=1, solver='sag', max_iter = 10000, random_state=241, n_jobs = -1,
                        multi_class='ovr', verbose = 10).fit(X_train,y_train)
yhat = LR.predict(X_test)
print('AUC, simple LR',roc_auc_score(y_test, yhat))
#print('My roc_auc, simple LR',roc_auc_my(y_test, yhat))
print('Accuracy, simple LR',accuracy_score(y_test, yhat))
print ('Time elapsed, simple LR:', datetime.datetime.now() - start_time_l)



#CatBoost, best los ='MultiClassOneVsAll',iter = 500, lr=1, depth =2
start_time_l = datetime.datetime.now()
cat = CatBoostClassifier(iterations=500,
                         depth=2,
                         learning_rate=1,
                         loss_function='MultiClassOneVsAll',
                         random_state=241,
                         logging_level='Silent')
#test_data = Pool(X_train, y_train)
cat.fit(X_cat_train,y_cat_train)
print('CatBoost')
#print('AUC',cat.eval_metrics(test_data))
#print('Accuracy',cat.score(test_data))
print(roc_auc_score(y_test, cat.predict(X_cat_test)))
print(accuracy_score(y_test, cat.predict(X_cat_test)))
print ('Time elapsed, CatBoost:', datetime.datetime.now() - start_time_l)

# cross-validation methods
scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}
kf = KFold(n_splits=5, random_state=241, shuffle=True)

"""
# Log Reg, best parameters: c = 1  solver = sag
start_time_l = datetime.datetime.now()
LR = LogisticRegression(C=1, solver='sag', max_iter = 10000, random_state=241)
cross_val_LR = cross_validate(LR, X_train, y_train, cv=kf, scoring=scoring, return_estimator=True, n_jobs=-1)
#proba = cross_val_predict(LR, X, y, cv=kf, method='predict_proba')
print('Log Reg')
print('AUC',cross_val_LR['test_AUC'])
print('Accuracy',cross_val_LR['test_Accuracy'])
cv_est_lr = cross_val_LR['estimator'][4]
print('AUC',roc_auc_score(y_test, cv_est_lr.predict(X_test)))
print('Accuracy',accuracy_score(y_test, cv_est_lr.predict(X_test)))
print ('Time elapsed, LR:', datetime.datetime.now() - start_time_l)
"""

"""
# Gradient Boost, cross-validation, best parameters: 3,40,1
start_time_l = datetime.datetime.now()
gbrc = GradientBoostingClassifier(max_depth=3, n_estimators=40,verbose=True, 
                                  random_state=241,learning_rate=1)
cross_val_gbrc = cross_validate(gbrc, X_cat_train, y_cat_train, cv=kf, 
                                scoring=scoring,return_estimator=True, n_jobs=-1)
cv_est_gb = cross_val_gbrc['estimator'][4]
print('Gradient Boost')
print('AUC',cross_val_gbrc['test_AUC'])
print('Accuracy',cross_val_gbrc['test_Accuracy'])
print(roc_auc_score(y_cat_test, cv_est_gb.predict(X_cat_test)))
print(accuracy_score(y_cat_test, cv_est_gb.predict(X_cat_test)))
print ('Time elapsed, GrBoost:', datetime.datetime.now() - start_time_l)

"""

"""#SGD
sgd = SGDClassifier(random_state=241)
cross_val_sgd = cross_validate(sgd, X_train, y_train, cv=kf, scoring=scoring,return_estimator=True)
cv_est_sgd = cross_val_sgd['estimator'][4]
print('SGD, stochastic')
print(roc_auc_score(y_test, cv_est_sgd.predict(X_test)))
print(accuracy_score(y_test, cv_est_sgd.predict(X_test)))
"""

"""#Random Forest
rnf = RandomForestClassifier(random_state=241)
cross_val_rnf = cross_validate(rnf, X_cat_train, y_cat_train, cv=kf, scoring=scoring,return_estimator=True, n_jobs=-1)
cv_est_rnf = cross_val_rnf['estimator'][4]
print('Random Forest')
print('AUC',cross_val_rnf['test_AUC'])
print('Accuracy',cross_val_rnf['test_Accuracy'])
print(roc_auc_score(y_cat_test, cv_est_rnf.predict(X_cat_test)))
print(accuracy_score(y_cat_test, cv_est_rnf.predict(X_cat_test)))
"""

"""#AdaBoost, 'SAMME', 200, lr=1
ada = AdaBoostClassifier(algorithm='SAMME',n_estimators=200,random_state=241)
cross_val_ada = cross_validate(ada, X_train, y_train, cv=kf, scoring=scoring,return_estimator=True, n_jobs=-1)
cv_est_ada = cross_val_ada['estimator'][4]
print('Ada')
print(roc_auc_score(y_test, cv_est_ada.predict(X_test)))
print(accuracy_score(y_test, cv_est_ada.predict(X_test)))
"""

"""
#Ensemble classification
rnf = RandomForestClassifier(random_state=241)
LR = LogisticRegression(C=1, solver='sag', max_iter = 10000, random_state=241, n_jobs = -1)
gbrc = GradientBoostingClassifier(max_depth=3, n_estimators=40,verbose=True, 
                                  random_state=241,learning_rate=1)
svc = LinearSVC(max_iter = 1000, random_state=241, C =1)
sgd = SGDClassifier(loss='huber', random_state=241)
DTreeC = DecisionTreeClassifier(max_depth = 5, random_state=241)

ensemble = VotingClassifier(estimators=[('Random Forest', rnf),('Log Reg', LR),('GradientBoost', gbrc)], 
                            voting='soft',n_jobs=-1).fit(X_train,y_train)

print('The accuracy Ensamble:',ensemble.score(X_test,y_test))
for clf in ensemble.named_estimators_.keys():
    print(ensemble.named_estimators_[clf].score(X_test,y_test))
"""

"""
result_LR=cat.predict_proba(XX)[:,1]
np.savetxt("LR.csv", result_LR, delimiter=",")

result_LR=cv_est_lr.predict_proba(XX)[:,1]
np.savetxt("LR_cv.csv", result_LR, delimiter=",")

"""
