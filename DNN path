import pandas as pd
import time
import datetime
import numpy as np
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn import preprocessing

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
from torch.utils.data import Dataset, DataLoader, random_split

from math import sqrt 
import math

torch.manual_seed(0)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# timer ON
start_time = datetime.datetime.now()

# data loading
#df = pd.read_csv('ML/train_features.csv', index_col='match_id_hash')
#df = pd.read_csv('new_features_train.csv', index_col='match_id_hash')
df = pd.read_csv('buyb_towers_train.csv', index_col='match_id_hash')
#df = pd.read_csv('items_all_train.csv', index_col='match_id_hash')

miscast = pd.read_csv('miscast_full.csv', index_col='match_id_hash')
df_test = pd.read_csv('ML/test_features.csv', index_col='match_id_hash')
df_target = pd.read_csv('ML/train_targets.csv', index_col='match_id_hash')

df = pd.concat([df,df_target['radiant_win']], axis=1)

def sigmoid1(x):
    return 1/(1+math.exp(-x))
# preprocessing

# doubles train data based on symmetry between radiant and dire teams
def data_doubling(df):
    df2 = df.copy(deep=True)
    df2['radiant_win']=df2['radiant_win'].map({True: False, False: True})
    df2_col =df2.columns.tolist()
    j = 0
    for col in df2.columns:
        for i in range(5):
            if col.count('r%d' %(i+1))>0: 
                col1 = col.replace('r%d' %(i+1),'d%d' %(i+1))
                df2_col[j]=col1
            elif col.count('d%d' %(i+1))>0:
                col1 = col.replace('d%d' %(i+1),'r%d' %(i+1))
                df2_col[j]=col1
        j+=1
    for col in df2.columns:
        for i in range(5):
            if col == ('r%d_x' %i) or col == ('r%d_y' %i) or col == ('d%d_x' %i) or col == ('d%d_y' %i):
                df2[col]=df2[col].apply(lambda x: 256-x)
    df2.columns = df2_col           
    temp_index = list(df2.index)
    temp_index = [x+'zzz' for x in temp_index]
    df2.index = temp_index
    df = pd.concat([df,df2])
    return df

# add to data team fields based on individual player fields
def add_team_fields(df, fields):
    X_fields = np.zeros((df.shape[0], len(fields)*2))
    ii = 0
    for i in df.index:
        field_counter = 0
        for field in fields:
            sumr = 0
            sumd = 0
            for p in range(5):
                sumr += df.loc[i, 'r%d' % (p+1)+field]
                sumd += df.loc[i, 'd%d' % (p+1)+field]
            X_fields[ii, field_counter] = sumr/(df.loc[i,'game_time']+1)
            X_fields[ii, field_counter+len(fields)] = sumd/(df.loc[i,'game_time']+1)
      #  if field == '_gold': X_fields_train[ii, len(fields)] = sumr-sumd        
      #  if field == '_xp': X_fields_train[ii, len(fields)+1] = sumr-sumd        
            field_counter+=1 
        ii+=1
    for i in range(len(fields)):
        df['team_radiant%s' %fields[i]] =X_fields[:,i]
        df['team_dire%s' %fields[i]] =X_fields[:,i+len(fields)]

# one-hot encoding of hero id +1 for radiant, -1 for dire    
def heroes_encoding(df):
    N = 121
    X_pick = np.zeros((df.shape[0], N))
    ii=0
    for i in df.index:
        for p in range(5):
            X_pick[ii, int(df.loc[i, 'r%d_hero_id' % (p+1)])] = 1
            X_pick[ii, int(df.loc[i, 'd%d_hero_id' % (p+1)])] = -1
        ii+=1
    for i in range(N):
        df['hero_%d' % i]=X_pick[:,i]
    for i in range(5):
        del df['r%d_hero_id' % (i+1)]
        del df['d%d_hero_id' % (i+1)]
        
def data_split_NN(df,fields):
    df_team=pd.DataFrame()
    df_team=df.iloc[:,0:5].copy()
    X_fields = np.zeros((df.shape[0], len(fields)*2))
    ii = 0
    for i in df.index:
        field_counter = 0
        for field in fields:
            sumr = 0
            sumd = 0
            for p in range(5):
                sumr += df.loc[i, 'r%d' % (p+1)+field]
                sumd += df.loc[i, 'd%d' % (p+1)+field]
            X_fields[ii, field_counter] = sumr/(df.loc[i,'game_time']+1)
            X_fields[ii, field_counter+len(fields)] = sumd/(df.loc[i,'game_time']+1)
      #  if field == '_gold': X_fields_train[ii, len(fields)] = sumr-sumd        
      #  if field == '_xp': X_fields_train[ii, len(fields)+1] = sumr-sumd        
            field_counter+=1 
        ii+=1
    for i in range(len(fields)):
        df_team['team_radiant%s' %fields[i]] =X_fields[:,i]
        df_team['team_dire%s' %fields[i]] =X_fields[:,i+len(fields)]
    return df_team

def data_split_gtime(df, time_zones):
    
    df1 = df.loc[df[df.game_time<time_zones[0]].index].copy(deep=True)
    df = df.drop(df[df.game_time<time_zones[0]].index)
    df2 = df.loc[df[df.game_time<time_zones[1]].index].copy(deep=True)
    df = df.drop(df[df.game_time<time_zones[1]].index)
    df3 = df.loc[df[df.game_time<time_zones[2]].index].copy(deep=True)
    df = df.drop(df[df.game_time<time_zones[2]].index)
    df4 = df.loc[df[df.game_time<time_zones[3]].index].copy(deep=True)
    df = df.drop(df[df.game_time<time_zones[3]].index)
    return df1,df2,df3,df4

#DNN setup

""" list for fields to optimize
['_kills', '_deaths', '_gold', '_lh', 
 '_xp', '_health', '_max_health', '_max_mana', 
 '_level', '_camps_stacked', '_rune_pickups', '_teamfight_participation', 
 '_towers_killed', '_roshans_killed', '_obs_placed','_sen_placed',
 '_stuns', '_assists', '_denies', '_x', 
 '_y', '_creeps_stacked', '_firstblood_claimed']
# special treat ['_gold','_xp','_towers_killed']"""

# fileds combining, train
fields = ['_kills', '_deaths', '_gold', '_lh', '_xp', '_health', '_max_health', '_max_mana', 
 '_level', '_camps_stacked', '_rune_pickups', '_teamfight_participation', '_towers_killed', 
 '_roshans_killed', '_obs_placed','_sen_placed',
 '_stuns', '_assists', '_denies', '_x', '_y', '_creeps_stacked', '_firstblood_claimed']

# build Dataset
class Dota2(Dataset):
    def __init__(self, images, labels=None, transforms=None):
        self.X = torch.from_numpy(images).type(torch.FloatTensor)
        self.y = torch.from_numpy(labels).type(torch.LongTensor)
        self.transforms = transforms
    def __len__(self):
        return (len(self.X))
    def __getitem__(self, i):
        data = self.X[i]
        if self.transforms:
            data = self.transforms(data)
        if self.y is not None:
            return (data, self.y[i])
        else:
            return data

# Create the model class using sigmoid as the activation function
class Net(nn.Module):
    # Constructor
    def __init__(self, D_in, H1, H2, D_out,p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(D_in, H1)
        self.linear2 = nn.Linear(H1, H2)
        self.linear3 = nn.Linear(H2, D_out)
    # Prediction
    def forward(self,x):
        x = torch.sigmoid(self.drop(self.linear1(x))) 
        x = torch.sigmoid(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x

# Train the model
def train(model, criterion, train_loader, validation_loader, optimizer, test_data, epochs=100, test=False):
    i = 0
    useful_stuff = {'training_loss': [], 'validation_accuracy': [],
                    'AUC_ROC':[], 
                    'y_pred':[],
                    'y_train_pass':[],'y_train':[],
                    'y_test_pass':[],'y_test':[],}  
# model train    
    for epoch in range(epochs):
        for i, (x, y) in enumerate(train_loader):
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            z = model(x)
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            useful_stuff['training_loss'].append(loss.data.item())
        correct = 0
# model validation
        for x, y in validation_loader:
            x, y = x.to(device), y.to(device)
            z = model(x)
            _, label = torch.max(z, 1)
            correct += (label == y).sum().item()
            # ROC_AUC
            roc = roc_auc_score(y.cpu().detach().numpy(),label.cpu().detach().numpy())

        useful_stuff['AUC_ROC'].append(roc*100)
        accuracy = 100 * (correct / len(validation_dataset))
        useful_stuff['validation_accuracy'].append(accuracy)
      #  print('Epoch',epoch,'accuracy',accuracy)
      #  print('Epoch',epoch,'ROC_AUC',roc)
      #  print('Epoch',epoch,'loss',loss.data.item())
    if test:
# prediction for new data       
            for x,y in test_data:
                x = x.to(device)
                z = model(x)
                _, label = torch.max(z, 1)
                Softmax_fn = nn.Softmax(dim=-1)
                Probability = Softmax_fn(z)
                useful_stuff['y_pred'].append(Probability.cpu().detach().numpy())
# train\test split predictions
            for x,y in train_loader:
                x = x.to(device)
                z = model(x)
                _, label = torch.max(z, 1)
                Softmax_fn = nn.Softmax(dim=-1)
                Probability = Softmax_fn(z)
                useful_stuff['y_train_pass'].append(Probability.cpu().detach().numpy())
                useful_stuff['y_train'].append(y.cpu().detach().numpy())    
            for x,y in validation_loader:
                x = x.to(device)
                z = model(x)
                _, label = torch.max(z, 1)
                Softmax_fn = nn.Softmax(dim=-1)
                Probability = Softmax_fn(z)
                useful_stuff['y_test_pass'].append(Probability.cpu().detach().numpy())
                useful_stuff['y_test'].append(y.cpu().detach().numpy())    
    return useful_stuff

# data processing, normalization and split
df=data_doubling(df)

#del df['radiant_win']
#add_team_fields(df,fields)
heroes_encoding(df)

#_,_,_,df = data_split_gtime(df,[300,600,2000,5000])
y = df['radiant_win'].map({True: 1, False: 0})
del df['radiant_win']

df_team = data_split_NN(df,fields)
df.fillna(value=0, inplace=True)
df_team.fillna(value=0, inplace=True)

#data normalization
X = preprocessing.StandardScaler().fit(df).transform(df)
X_team = preprocessing.StandardScaler().fit(df_team).transform(df_team)

input_dim = X.shape[1]
input_dim_t = X_team.shape[1]

# Datasets
X_total = np.concatenate((X_team, X, np.column_stack(y).T), axis=1)

split_train, split_validation = train_test_split(X_total)
#X_test = Dota2(X2,np.zeros((X2.shape[0],1)))
#X_team_test = Dota2(X2_team,np.zeros((X2_team.shape[0],1)))

train_dataset = Dota2(split_train[:,X_team.shape[1]:-1],split_train[:,-1])
team_train_dataset = Dota2(split_train[:,0:X_team.shape[1]],split_train[:,-1])
validation_dataset = Dota2(split_validation[:,X_team.shape[1]:-1],split_validation[:,-1])
team_validation_dataset = Dota2(split_validation[:,0:X_team.shape[1]],split_validation[:,-1])

# Create the training data loader and validation data loader object
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1000, shuffle=False)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=15000, shuffle=False)
#test_loader = torch.utils.data.DataLoader(dataset=X_test, batch_size=10000, shuffle=False)

team_train_loader = torch.utils.data.DataLoader(dataset=team_train_dataset, batch_size=1000, shuffle=False)
team_validation_loader = torch.utils.data.DataLoader(dataset=team_validation_dataset, batch_size=15000, shuffle=False)
#team_test_loader = torch.utils.data.DataLoader(dataset=X_team_test, batch_size=10000, shuffle=False)

print ('Time elapsed, data prep:', datetime.datetime.now() - start_time)

# Set the parameters for create the model
input_dim = input_dim
output_dim = 2
cust_epochs = 100
cust_epochs_t = 200
cust_epochs_f = 200
learning_rate = 1 #best lr
dim_b = 100 
dim_t = 100
dim_f = 100
#dropout
p = 0

# Create the criterion function
criterion = nn.CrossEntropyLoss()

# train BASE DATA model, 2 hidden layers optimal
model = Net(input_dim, dim_b, dim_b, output_dim,p=p)
model.to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
training_results = train(model, criterion, train_loader, validation_loader, optimizer,
                         validation_loader, epochs=cust_epochs, test=True)

# train TEAM DATA model, 2 hidden layers optimal
model_t = Net(input_dim_t, dim_t, dim_t, output_dim,p=p)
model_t.to(device)
optimizer = torch.optim.SGD(model_t.parameters(), lr=learning_rate)
training_results_t = train(model_t, criterion, team_train_loader, team_validation_loader, optimizer,
                         team_validation_loader, epochs=cust_epochs_t, test=True)

# train COMPOSIT model
# Train data from BASE and TEAM models
Train_Data = pd.DataFrame(np.concatenate(training_results['y_train_pass']))
Train_Data_T = pd.DataFrame(np.concatenate(training_results_t['y_train_pass']))
frames = [Train_Data,Train_Data_T]
Train_X = pd.concat(frames, axis=1)

"""Train_Data_y = pd.DataFrame(np.concatenate(training_results['y_train']))
Train_Data_Ty = pd.DataFrame(np.concatenate(training_results_t['y_train']))
frames = [Train_Data_y,Train_Data_Ty]
Train_y = pd.concat(frames, axis=1)"""

Train_y = pd.DataFrame(np.concatenate(training_results['y_train']))
Train_final = Dota2(Train_X[0].values,Train_y.values.squeeze())

# Validation data from BASE and TEAM models
Test_Data = pd.DataFrame(np.concatenate(training_results['y_test_pass']))
Test_Data_T = pd.DataFrame(np.concatenate(training_results_t['y_test_pass']))
frames = [Test_Data,Test_Data_T]
Test_X = pd.concat(frames, axis=1)

Test_y = pd.DataFrame(np.concatenate(training_results['y_test']))
Test_final = Dota2(Test_X[0].values,Test_y.values.squeeze())

# TRUE test data from BASE and TEAM models
TTest_Data = pd.DataFrame(np.concatenate(training_results['y_pred']))
TTest_Data_T = pd.DataFrame(np.concatenate(training_results_t['y_pred']))
frames = [TTest_Data,TTest_Data_T]
TTest_X = pd.concat(frames, axis=1)

#TTest_y = np.zeros((X2.shape[0],1))
#TTest_final = Dota2(TTest_X[0].values,TTest_y.squeeze())

# load Train\Test to DataLoader
train_loader_f = torch.utils.data.DataLoader(dataset=Train_final, batch_size=1000, shuffle=False)
validation_loader_f = torch.utils.data.DataLoader(dataset=Test_final, batch_size=15000, shuffle=False)
#test_loader_f = torch.utils.data.DataLoader(dataset=TTest_final, batch_size=10000, shuffle=False)

# final composit model training
model_c = Net(2, dim_f, dim_f, output_dim,p=p)
model_c.to(device)
optimizer = torch.optim.SGD(model_c.parameters(), lr=learning_rate)
training_results_f = train(model_c, criterion, train_loader_f, validation_loader_f, optimizer,
                         validation_loader_f, epochs=cust_epochs_f, test=True)

"""# Compare the training loss
plt.plot(training_results['training_loss'], label='2layer')
plt.ylabel('loss')
plt.title('training loss iterations')
plt.legend()
plt.show()"""

# Roc and Accuracy Base model
plt.title('Base model')
plt.plot(training_results['validation_accuracy'], label='Accuracy')
plt.plot(training_results['AUC_ROC'], label='AUC_ROC')
plt.ylabel('validation accuracy')
plt.xlabel('Iteration')   
plt.legend()
plt.show()

# Roc and Accuracy Team model
plt.title('Team model')
plt.plot(training_results_t['validation_accuracy'], label='Accuracy_team')
plt.plot(training_results_t['AUC_ROC'], label='AUC_ROC_team')
plt.ylabel('validation accuracy')
plt.xlabel('Iteration')   
plt.legend()
plt.show()

# Roc and Accuracy Final model
plt.title('Final model')
plt.plot(training_results_f['validation_accuracy'], label='Accuracy_final')
plt.plot(training_results_f['AUC_ROC'], label='AUC_ROC_final')
plt.ylabel('validation accuracy')
plt.xlabel('Iteration')   
plt.legend()
plt.show()

"""# Roc and Accuracy loss Base model
plt.plot(training_results['validation_accuracy'], label='Accuracy')
plt.plot(training_results['AUC_ROC'], label='AUC_ROC')
plt.plot(training_results_t['validation_accuracy'], label='Accuracy_team')
plt.plot(training_results_t['AUC_ROC'], label='AUC_ROC_team')
plt.plot(training_results_f['validation_accuracy'], label='Accuracy_final')
plt.plot(training_results_f['AUC_ROC'], label='AUC_ROC_final')
plt.ylabel('validation accuracy')
plt.xlabel('Iteration')   
plt.legend()
plt.show()"""


#result_NN = training_results_f['y_pred'][0][:,1]
#np.savetxt("LR.csv", result_NN, delimiter=",")

print ('Time elapsed, total:', datetime.datetime.now() - start_time)
